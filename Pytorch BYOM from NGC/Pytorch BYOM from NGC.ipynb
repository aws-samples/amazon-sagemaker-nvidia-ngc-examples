{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can download an NVIDIA model from torchhub\n",
    "\n",
    "#### ... and then compress it as a tar.gz file, and use this for deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-04-24 15:54:51--  https://api.ngc.nvidia.com/v2/models/nvidia/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt\n",
      "Resolving api.ngc.nvidia.com (api.ngc.nvidia.com)... 54.71.37.245, 52.27.83.248\n",
      "Connecting to api.ngc.nvidia.com (api.ngc.nvidia.com)|54.71.37.245|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 \n",
      "Location: https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt?response-content-disposition=attachment%3B%20filename%3D%22nvidia_ssdpyt_fp32_190826.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIA7U%2BXJ6mzT8tb8Mo9lUxGOLx8rNi1nemb0LG%2Fo%2FFtPPAiEAsQ26nRd8H5Pnzn6LSdyWyweZaga8RmUNhQ0hx%2BOJiKIqtAMIeRACGgw3ODkzNjMxMzUwMjciDJq4RK1xUSe5HJDxvCqRA1ZFlF2tnV3TPtE%2BRIJsViqDJwCtFD7HEiyKFCoxLgMBI%2F%2FMQnXWxfJmcepE5tYhgCsSHReB560xcqfXAwdOKCIUZS5ZOdeuJoLWCQZ9Q9WDSZmswkAaqVCuqKpTkqGEzNEQaA6EzU7i8xVmH3PbW4nm8lc7%2BHLaDKTtPGnvAg1ZEY67iG4cRFj2eh%2FK%2BKzyanH631Lts0XwBH0XB3iZCvcG123taYgdA5IGKUNArs5G7nAceNLLL8UlYgcwWtCO6ufqvUAXwE7si%2Bk8sOmMRfmOucQCO5q3fVGJiGqEE2cqS7P8gLg6YQf6aTu4JqlKWVgpUhw5Mb8q%2FgD8rllY5yQd4KqbrsYfJdguA%2BTCX78oPqzzXW%2FqNfvTQkd%2BCyuxYuaH2aH0arfXNtpwvd5AVF8FXFA0NcmP3KhYLQRc9%2BqigXXYWO98N%2BCNUvxR9DIDOrrWQ1%2FOcRgcfrsFzr%2BPKhhaaUnZ3OMsZhMUvqvyXvGhN16L6%2Bi%2FAtpiWvJgtNJQXrF2pjEUhBG%2FDzuA331YFuqOMPSLjPUFOusBJZHp2%2FbOA%2BaHULxY5IbRCnOIv2rA1hAVMTKmzveNScheKx35gYKt8Mttq4kF%2FJ6REH9cN8nTFv4aCpGtpRWJ2e2K7sm1jppbiOVhDaScNC4%2B3WwGMLVyoAhjKfQdbuXk%2BdlxFzlSVMKMTUfKqxnQgwoNoXmmzBPKugFSTM7aAUrr4lON%2BTRWXb4uV%2BO2ZZVFCkCVzF5PKaVdK4huSpLdSsQfIc0x3RsbNd%2FpXGhbzZDkdg7SqdyCOxm7cteeGg2KtLR5dxM%2F6%2BWsny2BprI6rd45BNhq6U6Qilmwvi1TpcH4%2FyyM43thXulr1w%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200424T155452Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZ4SUZT2AM%2F20200424%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=544cb7391a782eb7ac108b152540440f2860a53ba51e9147d9ad2f3f15c99405 [following]\n",
      "--2020-04-24 15:54:52--  https://s3.us-west-2.amazonaws.com/prod-model-registry-ngc-bucket/org/nvidia/models/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt?response-content-disposition=attachment%3B%20filename%3D%22nvidia_ssdpyt_fp32_190826.pt%22&response-content-type=application%2Foctet-stream&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEFAaCXVzLXdlc3QtMiJHMEUCIA7U%2BXJ6mzT8tb8Mo9lUxGOLx8rNi1nemb0LG%2Fo%2FFtPPAiEAsQ26nRd8H5Pnzn6LSdyWyweZaga8RmUNhQ0hx%2BOJiKIqtAMIeRACGgw3ODkzNjMxMzUwMjciDJq4RK1xUSe5HJDxvCqRA1ZFlF2tnV3TPtE%2BRIJsViqDJwCtFD7HEiyKFCoxLgMBI%2F%2FMQnXWxfJmcepE5tYhgCsSHReB560xcqfXAwdOKCIUZS5ZOdeuJoLWCQZ9Q9WDSZmswkAaqVCuqKpTkqGEzNEQaA6EzU7i8xVmH3PbW4nm8lc7%2BHLaDKTtPGnvAg1ZEY67iG4cRFj2eh%2FK%2BKzyanH631Lts0XwBH0XB3iZCvcG123taYgdA5IGKUNArs5G7nAceNLLL8UlYgcwWtCO6ufqvUAXwE7si%2Bk8sOmMRfmOucQCO5q3fVGJiGqEE2cqS7P8gLg6YQf6aTu4JqlKWVgpUhw5Mb8q%2FgD8rllY5yQd4KqbrsYfJdguA%2BTCX78oPqzzXW%2FqNfvTQkd%2BCyuxYuaH2aH0arfXNtpwvd5AVF8FXFA0NcmP3KhYLQRc9%2BqigXXYWO98N%2BCNUvxR9DIDOrrWQ1%2FOcRgcfrsFzr%2BPKhhaaUnZ3OMsZhMUvqvyXvGhN16L6%2Bi%2FAtpiWvJgtNJQXrF2pjEUhBG%2FDzuA331YFuqOMPSLjPUFOusBJZHp2%2FbOA%2BaHULxY5IbRCnOIv2rA1hAVMTKmzveNScheKx35gYKt8Mttq4kF%2FJ6REH9cN8nTFv4aCpGtpRWJ2e2K7sm1jppbiOVhDaScNC4%2B3WwGMLVyoAhjKfQdbuXk%2BdlxFzlSVMKMTUfKqxnQgwoNoXmmzBPKugFSTM7aAUrr4lON%2BTRWXb4uV%2BO2ZZVFCkCVzF5PKaVdK4huSpLdSsQfIc0x3RsbNd%2FpXGhbzZDkdg7SqdyCOxm7cteeGg2KtLR5dxM%2F6%2BWsny2BprI6rd45BNhq6U6Qilmwvi1TpcH4%2FyyM43thXulr1w%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20200424T155452Z&X-Amz-SignedHeaders=host&X-Amz-Expires=3600&X-Amz-Credential=ASIA3PSNVSIZ4SUZT2AM%2F20200424%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=544cb7391a782eb7ac108b152540440f2860a53ba51e9147d9ad2f3f15c99405\n",
      "Resolving s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)... 52.218.208.112\n",
      "Connecting to s3.us-west-2.amazonaws.com (s3.us-west-2.amazonaws.com)|52.218.208.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 183409492 (175M) [application/octet-stream]\n",
      "Saving to: ‘nvidia_ssdpyt_fp32_190826.pt’\n",
      "\n",
      "nvidia_ssdpyt_fp32_ 100%[===================>] 174.91M  20.3MB/s    in 9.0s    \n",
      "\n",
      "2020-04-24 15:55:01 (19.4 MB/s) - ‘nvidia_ssdpyt_fp32_190826.pt’ saved [183409492/183409492]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://api.ngc.nvidia.com/v2/models/nvidia/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "import tarfile\n",
    "import torch\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket() # can replace with your own s3 bucket! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n",
    "    archive.add('nvidia_ssdpyt_fp32_190826.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpath = sess.upload_data(\n",
    "    path='model.tar.gz', bucket=bucket,\n",
    "    key_prefix='sagemaker-pytorch/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transform_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transform_script.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision.models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, backbone='resnet50', backbone_path=None):\n",
    "        super().__init__()\n",
    "        if backbone == 'resnet18':\n",
    "            backbone = resnet18(pretrained=not backbone_path)\n",
    "            self.out_channels = [256, 512, 512, 256, 256, 128]\n",
    "        elif backbone == 'resnet34':\n",
    "            backbone = resnet34(pretrained=not backbone_path)\n",
    "            self.out_channels = [256, 512, 512, 256, 256, 256]\n",
    "        elif backbone == 'resnet50':\n",
    "            backbone = resnet50(pretrained=not backbone_path)\n",
    "            self.out_channels = [1024, 512, 512, 256, 256, 256]\n",
    "        elif backbone == 'resnet101':\n",
    "            backbone = resnet101(pretrained=not backbone_path)\n",
    "            self.out_channels = [1024, 512, 512, 256, 256, 256]\n",
    "        else:  # backbone == 'resnet152':\n",
    "            backbone = resnet152(pretrained=not backbone_path)\n",
    "            self.out_channels = [1024, 512, 512, 256, 256, 256]\n",
    "        if backbone_path:\n",
    "            backbone.load_state_dict(torch.load(backbone_path))\n",
    "\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:7])\n",
    "\n",
    "        conv4_block1 = self.feature_extractor[-1][0]\n",
    "\n",
    "        conv4_block1.conv1.stride = (1, 1)\n",
    "        conv4_block1.conv2.stride = (1, 1)\n",
    "        conv4_block1.downsample[0].stride = (1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        return x\n",
    "\n",
    "class SSD300(nn.Module):\n",
    "    def __init__(self, backbone=ResNet('resnet50')):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feature_extractor = backbone\n",
    "\n",
    "        self.label_num = 81  # number of COCO classes\n",
    "        self._build_additional_features(self.feature_extractor.out_channels)\n",
    "        self.num_defaults = [4, 6, 6, 6, 4, 4]\n",
    "        self.loc = []\n",
    "        self.conf = []\n",
    "\n",
    "        for nd, oc in zip(self.num_defaults, self.feature_extractor.out_channels):\n",
    "            self.loc.append(nn.Conv2d(oc, nd * 4, kernel_size=3, padding=1))\n",
    "            self.conf.append(nn.Conv2d(oc, nd * self.label_num, kernel_size=3, padding=1))\n",
    "\n",
    "        self.loc = nn.ModuleList(self.loc)\n",
    "        self.conf = nn.ModuleList(self.conf)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _build_additional_features(self, input_size):\n",
    "        self.additional_blocks = []\n",
    "        for i, (input_size, output_size, channels) in enumerate(zip(input_size[:-1], input_size[1:], [256, 256, 128, 128, 128])):\n",
    "            if i < 3:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv2d(input_size, channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(channels, output_size, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "                    nn.BatchNorm2d(output_size),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                )\n",
    "            else:\n",
    "                layer = nn.Sequential(\n",
    "                    nn.Conv2d(input_size, channels, kernel_size=1, bias=False),\n",
    "                    nn.BatchNorm2d(channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(channels, output_size, kernel_size=3, bias=False),\n",
    "                    nn.BatchNorm2d(output_size),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                )\n",
    "\n",
    "            self.additional_blocks.append(layer)\n",
    "\n",
    "        self.additional_blocks = nn.ModuleList(self.additional_blocks)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        layers = [*self.additional_blocks, *self.loc, *self.conf]\n",
    "        for layer in layers:\n",
    "            for param in layer.parameters():\n",
    "                if param.dim() > 1: nn.init.xavier_uniform_(param)\n",
    "\n",
    "    # Shape the classifier to the view of bboxes\n",
    "    def bbox_view(self, src, loc, conf):\n",
    "        ret = []\n",
    "        for s, l, c in zip(src, loc, conf):\n",
    "            ret.append((l(s).view(s.size(0), 4, -1), c(s).view(s.size(0), self.label_num, -1)))\n",
    "\n",
    "        locs, confs = list(zip(*ret))\n",
    "        locs, confs = torch.cat(locs, 2).contiguous(), torch.cat(confs, 2).contiguous()\n",
    "        return locs, confs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "\n",
    "        detection_feed = [x]\n",
    "        for l in self.additional_blocks:\n",
    "            x = l(x)\n",
    "            detection_feed.append(x)\n",
    "\n",
    "        # Feature Map 38x38x4, 19x19x6, 10x10x6, 5x5x6, 3x3x4, 1x1x4\n",
    "        locs, confs = self.bbox_view(detection_feed, self.loc, self.conf)\n",
    "\n",
    "        # For SSD 300, shall return nbatch x 8732 x {nlabels, nlocs} results\n",
    "        return locs, confs\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    # run prediction\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_data)\n",
    "    pred_dict = {'pred1':pred[0].detach().cpu().numpy(),'pred2':pred[1].detach().cpu().numpy()}\n",
    "    return pred_dict\n",
    "        \n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SSD300(backbone=ResNet('resnet50'))\n",
    "    try:\n",
    "        model_weights = torch.load(os.path.join(model_dir, 'nvidia_ssdpyt_fp32_190826.pt'), map_location='cpu')['model']\n",
    "        model.to('cpu')\n",
    "        model.load_state_dict(model_weights)\n",
    "    except:\n",
    "        print('using fallback model loading')\n",
    "        os.system('wget https://api.ngc.nvidia.com/v2/models/nvidia/ssdpyt_fp32/versions/2/files/nvidia_ssdpyt_fp32_190826.pt')\n",
    "        model_weights = torch.load(os.path.join('nvidia_ssdpyt_fp32_190826.pt'), map_location='cpu')['model']\n",
    "        model.to('cpu')\n",
    "        model.load_state_dict(model_weights)\n",
    "    model.eval()\n",
    "    return model \n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if(request_content_type == 'application/x-npy'):\n",
    "        try:\n",
    "            input_data = np.frombuffer(request_body, dtype=int)\n",
    "        except:\n",
    "            input_data = np.array(request_body, dtype=int)\n",
    "    try:\n",
    "        input_data = torch.tensor(np.reshape(input_data,(1,3,300,300)), dtype=torch.float32, device=device) # this needs to be a torch tensor \n",
    "    except:\n",
    "        input_data = torch.tensor(np.reshape(input_data[16:],(1,3,300,300)), dtype=torch.float32, device=device) # this needs to be a torch tensor \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelpath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f688f709b4ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m pytorch_model = PyTorchModel(model_data=modelpath, role=role,\n\u001b[0m\u001b[1;32m      2\u001b[0m                              \u001b[0mentry_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'transform_script.py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              framework_version='1.4.0')\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelpath' is not defined"
     ]
    }
   ],
   "source": [
    "pytorch_model = PyTorchModel(model_data=modelpath, role=role,\n",
    "                             entry_point='transform_script.py',\n",
    "                             framework_version='1.4.0')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.m4.xlarge', initial_instance_count=1, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imread('https://upload.wikimedia.org/wikipedia/commons/2/25/Postmen_Office_Room.jpg')\n",
    "img  = resize(img, (300,300,3))\n",
    "img = np.array(img, dtype=np.int64)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(img.tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also, you can download the model from torchhub with this API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ec2-user/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /home/ec2-user/.cache/torch/checkpoints/resnet50-19c8e357.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcab6f7575f455b98605a27acd6a435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/ssdpyt_fp32/versions/1/files/nvidia_ssdpyt_fp32_20190225.pt\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "precision = 'fp32'\n",
    "ssd_model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', model_math=precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or better, download the model from torch hub on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transform_script_hub.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transform_script_hub.py\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from six import BytesIO\n",
    "import numpy as np\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_ssd', model_math='fp32',map_location='gpu')\n",
    "    return model\n",
    "                       \n",
    "def input_fn(request_body, request_content_type):\n",
    "    return torch.load(BytesIO(request_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp\n",
      "-----------------!"
     ]
    }
   ],
   "source": [
    "#PyTorchModel requires a non-empty, model_data file\n",
    "\n",
    "!echo \"tmp content\" > tmp\n",
    "!tar -zcvf ./tmp.tar.gz tmp\n",
    "pytorch_model = PyTorchModel(model_data = 'file://tmp.tar.gz',\n",
    "                             role=role,\n",
    "                             entry_point='./transform_script_hub.py',\n",
    "                             framework_version='1.4.0')\n",
    "\n",
    "predictor = pytorch_model.deploy(instance_type='ml.p3.2xlarge',\n",
    "                                 initial_instance_count=1,\n",
    "                                 endpoint_name='nvidia-ssd-pytorch-gpu2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "uris = [\n",
    "    'http://images.cocodataset.org/val2017/000000397133.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000037777.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000252219.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "inputs = [utils.prepare_input(uri) for uri in uris]\n",
    "tensor = utils.prepare_tensor(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.transpose((2, 0, 1)).reshape(1,image.shape[0],image.shape[1],image.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
